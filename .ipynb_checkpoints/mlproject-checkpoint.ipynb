{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column names which has missing values\n",
      "['_last_judgment_at', 'gender', 'gender:confidence', 'description', 'gender_gold', 'profile_yn_gold', 'tweet_coord', 'tweet_location', 'user_timezone']\n",
      "Male Profile Count 6194\n",
      "Female Profile Count 6700\n",
      "Brand Profile Count 5942\n",
      "['_unit_id', '_golden', '_unit_state', '_trusted_judgments', '_last_judgment_at', 'gender', 'gender:confidence', 'profile_yn', 'profile_yn:confidence', 'created', 'description', 'fav_number', 'gender_gold', 'link_color', 'name', 'profile_yn_gold', 'profileimage', 'retweet_count', 'sidebar_color', 'text', 'tweet_coord', 'tweet_count', 'tweet_created', 'tweet_id', 'tweet_location', 'user_timezone', 'text_cleaned', 'description_cleaned']\n",
      "Male Profile Count 4653\n",
      "Female Profile Count 5367\n",
      "Brand Profile Count 3784\n",
      "Using 1 feature\n",
      "feature: text_cleaned\n",
      "Naive Bayes\n",
      "0.5526203664252237\n",
      "\n",
      "Decision Tree\n",
      "0.48146570089475926\n",
      "\n",
      "Support Vector Machines\n",
      "0.5543246697912229\n",
      "\n",
      "feature: description_cleaned\n",
      "Naive Bayes\n",
      "0.5374946740519813\n",
      "\n",
      "Decision Tree\n",
      "0.48487430762675754\n",
      "\n",
      "Support Vector Machines\n",
      "0.537920749893481\n",
      "\n",
      "feature: sidebar_color\n",
      "Naive Bayes\n",
      "0.5511291009799745\n",
      "\n",
      "Decision Tree\n",
      "0.48487430762675754\n",
      "\n",
      "Support Vector Machines\n",
      "0.5570941627609715\n",
      "\n",
      "feature: link_color\n",
      "Naive Bayes\n",
      "0.5515551768214743\n",
      "\n",
      "Decision Tree\n",
      "0.47848317000426077\n",
      "\n",
      "Support Vector Machines\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "import pickle\n",
    "import string\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import itertools\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn import tree\n",
    "from sklearn import svm\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "import scipy.sparse as sp\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def init():\n",
    "    \"\"\"\n",
    "    This method is used to read and learn about our dataset.\n",
    "    Also, empty rows are dropped and the rows with target class value missing are dropped too.\n",
    "    :return: The data set to be used for Predictive Analysis\n",
    "    \"\"\"\n",
    "\n",
    "    # Reading the data from the CSV file using the latin1 encoding.\n",
    "    data_read = pd.read_csv(\"gender-classifier-DFE-791531 2.csv\", encoding='latin1')  # Dataset Size = 20050\n",
    "\n",
    "    # If all the attribute values are empty for any of the rows, we drop them.\n",
    "    data = data_read.dropna(how='all')          # After dropping, data set size is still 20050\n",
    "\n",
    "    # Checking the names of the columns/attributes which contains at least one null value\n",
    "    columns_containing_missing_values = data.columns[data.isnull().any()].tolist()\n",
    "    print(\"Column names which has missing values\")\n",
    "    print(columns_containing_missing_values)\n",
    "\n",
    "    # Since 'gender' is our target variable, we would like to have values for it.\n",
    "    # So, dropping all the rows which have no values for the 'gender' attribute.\n",
    "    data = data[data['gender'].notnull()]       # After dropping, dataset size = 19953 rows\n",
    "    # Also, dropping all the rows which have values as 'unknown' for the 'gender' attribute\n",
    "    data = data[data['gender'] != 'unknown']    # After dropping, dataset size = 18836 rows\n",
    "\n",
    "    male_profile_count = len(data[data['gender'] == 'male'])\n",
    "    print(\"Male Profile Count \" + str(male_profile_count))\n",
    "    female_profile_count = len(data[data['gender'] == 'female'])\n",
    "    print(\"Female Profile Count \" + str(female_profile_count))\n",
    "    brand_profile_count = len(data[data['gender'] == 'brand'])\n",
    "    print(\"Brand Profile Count \" + str(brand_profile_count))\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"\n",
    "    This method is used to clean the text involved in our data set which is mainly the tweet and profile description\n",
    "    which are known by the attribute names 'text' and 'description'.\n",
    "    Cleaning involves converting all text to lower case, stripping all punctuation strings before or after every word,\n",
    "    removing a HTML code, whitespaces, special characters and numbers found in between the text.\n",
    "    :param text: A string\n",
    "    :return: Cleaned string\n",
    "    \"\"\"\n",
    "    text = str(text).lower()\n",
    "    text = text.strip(string.punctuation)\n",
    "    text = re.sub(\"&amp;\", '', text)\n",
    "    text = re.sub(\"https\", '', text)\n",
    "    text = re.sub('\\W\\s', '', text)\n",
    "    text = re.sub('\\s,\\W', '', text)\n",
    "    text = re.sub('[.!@#$%^&*()_,:;/-]', '', text)\n",
    "    text = re.sub(\"\\d+\", '', text)\n",
    "\n",
    "    return text\n",
    "\n",
    "\n",
    "def check_confidence(data):\n",
    "    \"\"\"\n",
    "    This method is used to filter the data based on the gender confidence.\n",
    "    Gender confidence provides the percentage of confidence the judges had while manually assigning the\n",
    "    genders to the twitter profiles. So, we consider only those observations in which the gender confidence\n",
    "    is 100% or 1.\n",
    "    :param data: The data set to be filtered\n",
    "    :return: Filtered data set.\n",
    "    \"\"\"\n",
    "    gender_confident_data = data[data['gender:confidence'] == 1]  # Dataset size = 13926\n",
    "    return gender_confident_data\n",
    "\n",
    "\n",
    "def split_dataset_for_one_attribute(gender_confident_data, attribute='text_cleaned'):\n",
    "    \"\"\"\n",
    "    This method is used to separate the input and output attributes and divide the data sets into training and testing\n",
    "    data sets. Also, we only consider one of the attributes as input like the tweet or the profile description\n",
    "    to determine the gender.\n",
    "    :param gender_confident_data: The data which has 100% confident gender labels.\n",
    "    :param attribute: The attribute using which classification model has to be trained and tested.\n",
    "    :return: The training and testing data sets.\n",
    "    \"\"\"\n",
    "\n",
    "    # Converting text strings into a matrix of word token counts.\n",
    "    cv = CountVectorizer()\n",
    "    inputString = cv.fit_transform(gender_confident_data[attribute])\n",
    "    #cv = CountVectorizer()\n",
    "    #inputString = cv.fit_transform(gender_confident_data['sidebar_color'])\n",
    "    #print(inputString[1])\n",
    "    \n",
    "    # Encodes class labels from 0 to Num_of_classes-1\n",
    "    le = LabelEncoder()\n",
    "    outputString = le.fit_transform(gender_confident_data['gender'])\n",
    "\n",
    "    # Splitting the data such that 66% of the data is assigned as training data and the rest as the test data set.\n",
    "    input_train, input_test, output_train, output_test = train_test_split(inputString, outputString, train_size=0.66)\n",
    "\n",
    "    return input_train, output_train, input_test, output_test\n",
    "\n",
    "\n",
    "def combine_tweet_and_description(data):\n",
    "    \"\"\"\n",
    "    This method combines the text from both user's tweet and profile description and forms a new feature\n",
    "    to be used for classification of user's gender. Also, uses this feature and splits the data set into training\n",
    "    and testing sets.\n",
    "    :param data: The data set\n",
    "    :return: Training and Testing data sets.\n",
    "    \"\"\"\n",
    "    data['combined_text'] = data['text_cleaned'].str.cat(data['description_cleaned'], sep='')\n",
    "    gender_confident_data = check_confidence(data)\n",
    "\n",
    "    # Converting text strings into a matrix of word token counts.\n",
    "    cv = CountVectorizer()\n",
    "    inputString = cv.fit_transform(gender_confident_data['combined_text'])\n",
    "\n",
    "    # Encodes class labels from 0 to Num_of_classes-1\n",
    "    le = LabelEncoder()\n",
    "    outputString = le.fit_transform(gender_confident_data['gender'])\n",
    "\n",
    "    # Splitting the data such that 66% of the data is assigned as training data and the rest as the test data set.\n",
    "    input_train, input_test, output_train, output_test = train_test_split(inputString, outputString, train_size=0.66)\n",
    "    print(input_train[0])\n",
    "    return input_train, output_train, input_test, output_test\n",
    "\n",
    "\n",
    "def select_important_features(data):\n",
    "    \"\"\"\n",
    "    This method uses 5 most relevant user profile features that help determine the gender of twitter users.\n",
    "    We consider the tweet, profile description, sidebar color, link color and profile names.\n",
    "    :param data: Data set\n",
    "    :return: Training and Testing data sets.\n",
    "    \"\"\"\n",
    "\n",
    "    selected_attributes = ['text_cleaned', 'description_cleaned', 'sidebar_color', 'link_color', 'name']\n",
    "    filtered_data = pd.DataFrame(data, columns=selected_attributes)\n",
    "    output_data = data['gender']\n",
    "\n",
    "    # Converting text strings into a matrix of word token counts\n",
    "    cv = CountVectorizer()\n",
    "    inputString = sp.hstack(filtered_data.apply(lambda attribute: cv.fit_transform(attribute)))\n",
    "\n",
    "    # Encodes class labels from 0 to Num_of_classes-1\n",
    "    le = LabelEncoder()\n",
    "    outputString = le.fit_transform(output_data)\n",
    "\n",
    "    # Splitting the data such that 66% of the data is assigned as training data and the rest as the test data set.\n",
    "    input_train, input_test, output_train, output_test = train_test_split(inputString, outputString, train_size=0.66)\n",
    "    return input_train, output_train, input_test, output_test\n",
    "\n",
    "\n",
    "def select_features(data,attributes):\n",
    "    \"\"\"\n",
    "    This method uses 4 most relevant user profile features that help determine the gender of twitter users.\n",
    "    :param data: Data set\n",
    "    :return: Training and Testing data sets.\n",
    "    \"\"\"\n",
    "    filtered_data = pd.DataFrame(data, columns=attributes)\n",
    "    output_data = data['gender']\n",
    "\n",
    "    # Converting text strings into a matrix of word token counts\n",
    "    cv = CountVectorizer()\n",
    "    inputString = sp.hstack(filtered_data.apply(lambda attribute: cv.fit_transform(attribute)))\n",
    "\n",
    "    # Encodes class labels from 0 to Num_of_classes-1\n",
    "    le = LabelEncoder()\n",
    "    outputString = le.fit_transform(output_data)\n",
    "\n",
    "    # Splitting the data such that 66% of the data is assigned as training data and the rest as the test data set.\n",
    "    input_train, input_test, output_train, output_test = train_test_split(inputString, outputString, train_size=0.66)\n",
    "    return input_train, output_train, input_test, output_test\n",
    "    \n",
    "\n",
    "def train_and_test_model(In_train, Out_train, In_test, Out_test):\n",
    "    \"\"\"\n",
    "    This method is used to train and test different classifier models for the provided training and testing data sets.\n",
    "    It also prints out the accuracy and confusion matrices for each classifier.\n",
    "    :param In_train: Inputs for training\n",
    "    :param Out_train: Target values for Training\n",
    "    :param In_test: Inputs for Testing\n",
    "    :param Out_test: Target values for Testing\n",
    "    :return: The trained Naive Bayes Classifier for saving the model.\n",
    "    \"\"\"\n",
    "\n",
    "    # Naive Bayes Classifier\n",
    "    print(\"Naive Bayes\")\n",
    "    NB_classifier = MultinomialNB()\n",
    "    NB_classifier.fit(In_train, Out_train)\n",
    "    predictions = NB_classifier.predict(In_test)\n",
    "    print(NB_classifier.score(In_test, Out_test))\n",
    "    print()\n",
    "\n",
    "\n",
    "\n",
    "    # Decision Tree Classifier\n",
    "    print(\"Decision Tree\")\n",
    "    DT_classifier = tree.DecisionTreeClassifier()\n",
    "    DT_classifier.fit(In_train, Out_train)\n",
    "    predictions = DT_classifier.predict(In_test)\n",
    "    print(DT_classifier.score(In_test, Out_test))\n",
    "    print()\n",
    "\n",
    "\n",
    "    # Support Vector Machines\n",
    "    print(\"Support Vector Machines\")\n",
    "    SVM_Classifier = svm.SVC()\n",
    "    SVM_Classifier.fit(In_train, Out_train)\n",
    "    predictions = SVM_Classifier.predict(In_test)\n",
    "    print(SVM_Classifier.score(In_test, Out_test))\n",
    "    print()\n",
    "\n",
    "    return NB_classifier\n",
    "\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    The main method.\n",
    "    :return: None\n",
    "    \"\"\"\n",
    "\n",
    "    data = init()\n",
    "    data['text_cleaned'] = [clean_text(tweet) for tweet in data['text']]\n",
    "    data['description_cleaned'] = [clean_text(line) for line in data['description']]\n",
    "    print(list(data))\n",
    "    gender_confident_data = check_confidence(data)\n",
    "\n",
    "    male_profile_count = len(gender_confident_data[gender_confident_data['gender'] == 'male'])\n",
    "    print(\"Male Profile Count \" + str(male_profile_count))\n",
    "    female_profile_count = len(gender_confident_data[gender_confident_data['gender'] == 'female'])\n",
    "    print(\"Female Profile Count \" + str(female_profile_count))\n",
    "    brand_profile_count = len(gender_confident_data[gender_confident_data['gender'] == 'brand'])\n",
    "    print(\"Brand Profile Count \" + str(brand_profile_count))\n",
    "    \n",
    "    # Using 1 feature \n",
    "    print(\"Using 1 feature\")\n",
    "    features = ['text_cleaned', 'description_cleaned', 'sidebar_color', 'link_color','name']\n",
    "    for feature in features:\n",
    "        print(\"feature:\",feature)\n",
    "        X, Y, x, y = split_dataset_for_one_attribute(gender_confident_data)\n",
    "        train_and_test_model(X, Y, x, y)\n",
    "                \n",
    "    #Using 4 features\n",
    "    print(\"Using 4 features\")\n",
    "    attributes = [['text_cleaned', 'description_cleaned', 'sidebar_color', 'link_color'],\n",
    "                  ['text_cleaned', 'description_cleaned', 'sidebar_color',  'name'],\n",
    "                  ['text_cleaned', 'description_cleaned',  'link_color', 'name'],\n",
    "                  ['text_cleaned',  'sidebar_color', 'link_color', 'name'],\n",
    "                  ['description_cleaned', 'sidebar_color', 'link_color', 'name']]\n",
    "    for attribute in attributes:\n",
    "        print(\"features:\",attribute)\n",
    "        X, Y, x, y = select_features(gender_confident_data,attribute)\n",
    "        trained_model = train_and_test_model(X, Y, x, y)\n",
    "    \n",
    "    #Using 3 features \n",
    "    three_attributes = attributes = [[ 'sidebar_color', 'link_color','name'],\n",
    "                                     ['sidebar_color', 'link_color','text_cleaned'],\n",
    "                                     ['sidebar_color', 'link_color', 'description_cleaned'],\n",
    "                              ['text_cleaned',  'link_color', 'name'],\n",
    "                              ['text_cleaned', 'link_color', 'name'],\n",
    "                  ['description_cleaned','description_cleaned','name']]\n",
    "    for attribute in three_attributes:\n",
    "        print(attribute)\n",
    "        X, Y, x, y = select_features(gender_confident_data,attribute)\n",
    "        trained_model = train_and_test_model(X, Y, x, y)\n",
    "        \n",
    "    \n",
    "    # Using 2 features profile description and tweets description\n",
    "    print(\"Combining two features\")\n",
    "    X, Y, x, y = combine_tweet_and_description(data)\n",
    "    train_and_test_model(X, Y, x, y)\n",
    "    \n",
    "    # Using 5 features\n",
    "    X, Y, x, y = select_important_features(gender_confident_data)\n",
    "    trained_model = train_and_test_model(X, Y, x, y)\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
